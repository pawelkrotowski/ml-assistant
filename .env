# --- DB ---
DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/ml_assistant

# --- RAG / API ---
RAG_CITATIONS=0
MAX_CONTEXT_CHARS=1600

# --- LLM: wybór providera ---
# do lokalnego Qwena użyj "ollama"
LLM_PROVIDER=ollama
LLM_TIMEOUT=300

# --- OLLAMA (lokalny LLM) ---
# jeśli API i ollama lecą w tym samym docker-compose: host to nazwa serwisu "ollama"
OLLAMA_BASE_URL=http://ollama:11434

# sensowny default pod 4070 Ti (12 GB): 7B instruct
# (większe: qwen2.5:14b-instruct; uważaj na VRAM)
OLLAMA_MODEL=qwen2.5:7b-instruct

# liczba tokenów do wygenerowania (nie kontekst!)
OLLAMA_NUM_PREDICT=512

# okno kontekstu dla modelu (zależnie od wariantu; 8192 to bezpieczny start)
OLLAMA_NUM_CTX=8192

# dodatkowe opcje do silnika (opcjonalnie)
# przykładowo: {"top_p":0.9,"repeat_penalty":1.1}
OLLAMA_OPTIONS_JSON={}

# --- AZURE (opcjonalnie; używane gdy LLM_PROVIDER=azure lub jako provider embeddingów) ---
AZURE_OPENAI_API_KEY=your_key
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_DEPLOYMENT=gpt-4o-mini     # lub gpt-4o

# --- EMBEDDINGS ---
# wybierz skąd bierzesz wektory: azure | local
EMBED_PROVIDER=azure
# nazwa deploymentu w Azure (Twoja, nie nazwa modelu)
AZURE_EMBED_DEPLOYMENT=text-embedding-3-large
