services:
  db:
    image: pgvector/pgvector:pg16
    container_name: ml_db
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: ml_assistant
    volumes:
      - db-data:/var/lib/postgresql/data
      - ./db-init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d ml_assistant"]
      interval: 5s
      timeout: 5s
      retries: 20

  ollama:
    image: ollama/ollama:latest
    container_name: ml_ollama
    command: [ "serve" ]
    # ports: ["11434:11434"]   # mapuj tylko jeśli chcesz używać z hosta
    gpus: all
    volumes:
      - ollama:/root/.ollama
    environment:
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
      OLLAMA_KEEP_ALIVE: "4h"
    healthcheck:
      test: [ "CMD", "/bin/ollama", "ps" ]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 20s
    restart: unless-stopped


    # Jednorazowy init do pobrania modelu do współdzielonego wolumenu
  ollama-init:
    image: ollama/ollama:latest
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      OLLAMA_HOST: "http://ollama:11434"   # <— wskaż serwis ollama
    entrypoint: [ "/bin/ollama" ]
    command: [ "pull", "${OLLAMA_MODEL:-qwen2.5:7b-instruct}" ]
    volumes:
      - ollama:/root/.ollama
    restart: "no"

  api:
    image: python:3.11-slim
    container_name: ml_api
    working_dir: /app
    env_file: .env
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - RAG_CITATIONS=${RAG_CITATIONS}
      - MAX_CONTEXT_CHARS=${MAX_CONTEXT_CHARS}
      - LLM_PROVIDER=${LLM_PROVIDER}
      - LLM_TIMEOUT=${LLM_TIMEOUT}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - OLLAMA_MODEL=${OLLAMA_MODEL}
      - OLLAMA_NUM_PREDICT=${OLLAMA_NUM_PREDICT}
      - OLLAMA_NUM_CTX=${OLLAMA_NUM_CTX}
      - OLLAMA_OPTIONS_JSON=${OLLAMA_OPTIONS_JSON}
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
      - AZURE_OPENAI_API_VERSION=${AZURE_OPENAI_API_VERSION}
      - AZURE_OPENAI_DEPLOYMENT=${AZURE_OPENAI_DEPLOYMENT}
      - EMBED_PROVIDER=${EMBED_PROVIDER}
      - AZURE_EMBED_DEPLOYMENT=${AZURE_EMBED_DEPLOYMENT}
    volumes:
      - ./:/app
      - pip-cache:/root/.cache/pip
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_healthy
    command: [
      "bash","-lc",
      "apt-get update && apt-get install -y --no-install-recommends build-essential python3-dev && rm -rf /var/lib/apt/lists/* && pip install --upgrade pip && pip install --no-cache-dir -r requirements.txt -f https://download.pytorch.org/whl/cpu && uvicorn api.main:app --host 0.0.0.0 --port 8000 --workers 1 --proxy-headers"
    ]
    ports:
      - "8000:8000"

volumes:
  db-data:
  pip-cache:
  ollama:
